import{_ as n,c as t,o as d,aQ as r}from"./chunks/framework.CtsLwA2Q.js";const p=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"AILargeModel/Embedding.md","filePath":"AILargeModel/Embedding.md"}'),i={name:"AILargeModel/Embedding.md"};function a(o,e,m,l,g,s){return d(),t("div",null,e[0]||(e[0]=[r('<h2 id="embedding-模型-它是现代人工智能-特别是-rag、搜索、推荐等领域的基石技术。" tabindex="-1">Embedding 模型，它是现代人工智能，特别是 RAG、搜索、推荐等领域的基石技术。 <a class="header-anchor" href="#embedding-模型-它是现代人工智能-特别是-rag、搜索、推荐等领域的基石技术。" aria-label="Permalink to &quot;Embedding 模型，它是现代人工智能，特别是 RAG、搜索、推荐等领域的基石技术。&quot;">​</a></h2><h2 id="一、核心概念-什么是-embedding" tabindex="-1">一、核心概念：什么是 Embedding？ <a class="header-anchor" href="#一、核心概念-什么是-embedding" aria-label="Permalink to &quot;一、核心概念：什么是 Embedding？&quot;">​</a></h2><p>想象一下，我们要让计算机理解词语的意思。计算机不认识文字，只认识数字。Embedding 就是一种将<em><strong>离散的、非结构化的数据</strong></em>（如单词、句子、图片、音频）转换为<em><strong>连续的、数值化的向量</strong></em>的技术。</p><p>这个转换后的结果，就是一个 <em><strong>“向量”</strong></em>，它可以被看作是在一个高维空间中的一个<em><strong>点</strong></em> 或 <em><strong>箭头</strong></em>。</p><ul><li><p><em><strong>最关键的思想是</strong></em>： 在这个高维空间中，<em><strong>语义相近的物体，其向量的空间距离也更近</strong></em>。</p></li><li><p><em><strong>一个经典的例子</strong></em></p><ul><li>“国王”的向量 - “男人”的向量 + “女人”的向量 ≈ “女王”的向量</li><li>“巴黎”的向量 - “法国”的向量 + “意大利”的向量 ≈ “罗马”的向量</li></ul></li><li><p>这说明了 Embedding 模型捕捉到了词语之间的语义和语法关系。</p></li></ul><h2 id="二、embedding-是如何工作的" tabindex="-1">二、Embedding 是如何工作的？ <a class="header-anchor" href="#二、embedding-是如何工作的" aria-label="Permalink to &quot;二、Embedding 是如何工作的？&quot;">​</a></h2><p>它的训练过程通常基于一个简单的原理：<em><strong>“观其友，识其人”</strong></em>。</p><ul><li><em><strong>目标</strong></em>：模型通过学习大量的文本数据（如维基百科、网页等），学会预测一个词语在它的上下文中会出现。</li><li><em><strong>过程</strong></em>：模型会为每个词分配一个随机的初始向量。然后，它读取句子。例如，对于句子 <code>“猫坐在垫子上”</code><ul><li>模型的任务可能是：给定 “猫”、“坐在”，预测下一个词是 “垫子上”。</li><li>或者给定 “坐在”、“垫子上”，预测中间的词是 “猫”。</li></ul></li></ul>',8)]))}const c=n(i,[["render",a]]);export{p as __pageData,c as default};
