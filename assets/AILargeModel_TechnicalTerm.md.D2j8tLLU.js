import{_ as n,c as t,o as i,aQ as e}from"./chunks/framework.CtsLwA2Q.js";const c=JSON.parse('{"title":"AI大模型技术术语","description":"","frontmatter":{},"headers":[],"relativePath":"AILargeModel/TechnicalTerm.md","filePath":"AILargeModel/TechnicalTerm.md"}'),o={name:"AILargeModel/TechnicalTerm.md"};function l(a,r,s,g,p,d){return i(),t("div",null,r[0]||(r[0]=[e('<h1 id="ai大模型技术术语" tabindex="-1">AI大模型技术术语 <a class="header-anchor" href="#ai大模型技术术语" aria-label="Permalink to &quot;AI大模型技术术语&quot;">​</a></h1><h2 id="模型架构相关" tabindex="-1">模型架构相关 <a class="header-anchor" href="#模型架构相关" aria-label="Permalink to &quot;模型架构相关&quot;">​</a></h2><ul><li><p><strong>Transformer</strong>：基于自注意力机制（Self-Attention）的神经网络架构，是大模型（如GPT、BERT）的核心。</p></li><li><p><strong>Self-Attention（自注意力）</strong>：通过计算输入序列中所有位置的关联权重，动态捕捉上下文依赖。</p></li><li><p><strong>Multi-Head Attention（多头注意力）</strong>：将自注意力机制拆分为多个“头”，并行学习不同子空间的注意力模式。</p></li><li><p><strong>FFN（Feed-Forward Network）</strong>：Transformer中的全连接前馈网络，对每个位置独立进行非线性变换。</p></li><li><p><strong>Layer Normalization（层归一化）</strong>：对每一层的输出进行归一化，稳定训练过程。</p></li><li><p><strong>Positional Encoding（位置编码）</strong>：为输入序列添加位置信息，弥补Transformer缺乏时序感知的缺陷。</p></li></ul><h2 id="训练相关" tabindex="-1">训练相关 <a class="header-anchor" href="#训练相关" aria-label="Permalink to &quot;训练相关&quot;">​</a></h2><ul><li><p><strong>Pre-training（预训练）</strong>：在大规模无标注数据上训练模型，学习通用表示（如语言模型任务）。</p></li><li><p><strong>Fine-tuning（微调）</strong>：在预训练模型基础上，用特定任务的小规模标注数据进一步训练。</p></li><li><p><strong>Self-Supervised Learning（自监督学习）</strong>：通过设计辅助任务（如掩码语言模型）从无标注数据中自动生成监督信号。</p></li><li><p><strong>Loss Function（损失函数）</strong>：</p><ul><li><strong>Cross-Entropy Loss（交叉熵损失）</strong>：分类任务常用，衡量预测分布与真实分布的差异。</li><li><strong>Perplexity（困惑度）</strong>：语言模型的评估指标，越低表示模型预测越准确。</li></ul></li><li><p><strong>Optimizer（优化器）</strong>：</p><ul><li><strong>Adam/AdamW</strong>：自适应学习率优化算法，广泛用于大模型训练。</li></ul></li><li><p><strong>Learning Rate Scheduling（学习率调度）</strong>：动态调整学习率（如余弦退火、线性预热）。</p></li></ul><h2 id="参数与计算" tabindex="-1">参数与计算 <a class="header-anchor" href="#参数与计算" aria-label="Permalink to &quot;参数与计算&quot;">​</a></h2><ul><li><p><strong>Parameters（参数量）</strong>：模型可学习的权重数量（如GPT-3有1750亿参数）。</p></li><li><p><strong>FLOPs（浮点运算次数）</strong>：衡量模型计算复杂度。</p></li><li><p><strong>GPU Memory（显存占用）</strong>：训练/推理时模型占用的GPU内存，与参数量和批次大小相关。</p></li><li><p><strong>Distributed Training（分布式训练）</strong>：</p><ul><li><strong>Data Parallelism（数据并行）</strong>：将数据分片到多个设备，同步更新模型。</li><li><strong>Model Parallelism（模型并行）</strong>：将模型分片到多个设备（如Tensor Parallelism、Pipeline Parallelism）。</li><li><strong>ZeRO（Zero Redundancy Optimizer）</strong>：减少显存占用的分布式训练技术（DeepSpeed库实现）。</li></ul></li></ul><h2 id="关键技术与方法" tabindex="-1">关键技术与方法 <a class="header-anchor" href="#关键技术与方法" aria-label="Permalink to &quot;关键技术与方法&quot;">​</a></h2><ul><li><p>Prompt Engineering（提示工程）：设计输入提示（Prompt）以引导模型生成期望输出。</p></li><li><p><strong>Few-Shot Learning（少样本学习）</strong>：通过少量示例让模型理解任务，无需微调。</p></li><li><p><strong>RLHF（Reinforcement Learning from Human Feedback）</strong>：基于人类反馈的强化学习（如ChatGPT的训练流程）。</p></li><li><p><strong>LoRA（Low-Rank Adaptation）</strong>：通过低秩矩阵微调大模型，减少计算成本。</p></li><li><p><strong>KV Cache（Key-Value缓存）</strong>：推理时缓存注意力层的Key/Value，加速生成过程。</p></li><li><p><strong>KV Cache（Key-Value缓存）</strong>：推理时缓存注意力层的Key/Value，加速生成过程。</p></li></ul><h2 id="评估指标" tabindex="-1">评估指标 <a class="header-anchor" href="#评估指标" aria-label="Permalink to &quot;评估指标&quot;">​</a></h2><ul><li><p><strong>BLEU（Bilingual Evaluation Understudy）</strong>：机器翻译或文本生成任务的评估指标，基于n-gram匹配。</p></li><li><p><strong>ROUGE（Recall-Oriented Understudy for Gisting Evaluation）</strong>：文本摘要任务的评估指标，衡量重叠单元（如词、n-gram）。</p></li><li><p><strong>Accuracy/F1-Score</strong>：分类任务的常用指标。</p></li><li><p><strong>MMLU（Massive Multitask Language Understanding）</strong>：评估模型多任务知识理解的基准。</p></li></ul><h2 id="常见模型家族" tabindex="-1">常见模型家族 <a class="header-anchor" href="#常见模型家族" aria-label="Permalink to &quot;常见模型家族&quot;">​</a></h2><ul><li><p><strong>GPT（Generative Pre-trained Transformer）</strong>：自回归模型，擅长生成任务（如GPT-3、ChatGPT）。</p></li><li><p><strong>BERT（Bidirectional Encoder Representations）</strong>：双向编码器模型，擅长理解任务。</p></li><li><p><strong>T5（Text-to-Text Transfer Transformer）</strong>：将所有任务统一为文本到文本的格式。</p></li><li><p><strong>LLaMA（Meta开源模型）</strong>：基于Transformer的高效大模型系列。</p></li><li><p><strong>Mixture of Experts（MoE）</strong>：稀疏激活模型，如Google的Switch Transformer。</p></li></ul><h2 id="其他重要概念" tabindex="-1">其他重要概念 <a class="header-anchor" href="#其他重要概念" aria-label="Permalink to &quot;其他重要概念&quot;">​</a></h2><ul><li><p><strong>Tokenization（分词）</strong>：将文本转换为模型可处理的token（如Byte-Pair Encoding）。</p></li><li><p><strong>Temperature（温度参数）</strong>：控制生成多样性的超参数（低温度=确定性高，高温度=随机性高）。</p></li><li><p><strong>Top-k/Top-p采样</strong>：生成文本时筛选候选token的策略（如Top-p=核采样）。</p></li><li><p><strong>Hallucination（幻觉）</strong>：模型生成与事实不符的内容。</p></li></ul>',15)]))}const h=n(o,[["render",l]]);export{c as __pageData,h as default};
